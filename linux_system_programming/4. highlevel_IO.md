# 高度なファイルIO

- 前章で述べた通り、基本的に read/write の呼び出しはしたくない
- ユーザランドバッファ <-> カーネルランドバッファ <-> ディスクの効率が悪い
- よって様々なケースで如何に read/write を呼ばないか？という努力がされている

## scatter-gather IO

- ある程度高いレイヤのプログラミングでは、構造化されたデータに対して読み書きを行うことがある
- 例：HTTPResponse：status/body
  - クライアント側では読み出し、サーバ側では書き込み
- この際、愚直に read/write を発行してもよいが…
  - オーバーヘッドが大きい
  - ディスクアクセスの機会が増えると本当に遅くなる
- 該当のユースケースが多いので、メモリの連続的な領域に対する、複数のバッファを利用した読み出し/書き込みのシステムコールが実装された
- 
- *readv/writev*
- readv -> ファイルから連続した領域を読み出し、複数のバッファに格納する
- writev -> 複数のバッファからファイルに対して書き込みを行う
- IO 要求としては単一なのでシステムコール呼び出しの回数を削減できる

## Event Poll インターフェース

- ネットワーク黎明期
- サーバが管理するTCP接続の数は単調増加
- どんどん増加
- [C10K](https://ja.wikipedia.org/wiki/C10K%E5%95%8F%E9%A1%8C) 問題の発生

### 1接続1プロセス/スレッドの時代

```c
while (1) {
    int client_fd = accept(server_fd, ...);  // ブロック
    if (fork() == 0) {  // 接続ごとにプロセス生成
        handle_client(client_fd);
        exit(0);
    }
    close(client_fd);
}
```

https://httpd.apache.org/docs/current/ja/mod/prefork.html

- 接続のたびに fork を行うのでオーバーヘッドが馬鹿にならん
- 他の接続に関する任意の処理のたびにコンテキストスイッチが必要

### select の導入によりアクティブポーリングが実現

```c
fd_set readfds, writefds;
while (1) {
    FD_ZERO(&readfds);
    // 監視したいFDを毎回セット
    for (int i = 0; i < num_fds; i++) {
        FD_SET(fds[i], &readfds);
    }
    
    int ready = select(max_fd + 1, &readfds, &writefds, NULL, NULL);
    
    // 準備完了したFDを探す(全部チェック!)
    for (int i = 0; i < num_fds; i++) {
        if (FD_ISSET(fds[i], &readfds)) {
            handle_read(fds[i]);
        }
    }
}
```
https://linuxjm.sourceforge.io/html/LDP_man-pages/man2/select.2.html

- 確認できる fd のサイズは 1024 に制限される。
  - long のビットマスクで管理されるため。
  - 例：1ビット目が 1 なら fd = 1 は fd_set に含まれる。
- イテレーションのたびに fd_set の構築が必要。
  - fd が大きいほどメモリを利用する
- select の結果は fd の数だけ each 文を回し、イベントの確認が必要になる。

### poll により fd の上限値問題を解消

```c
struct pollfd fds[MAX_CONNECTIONS];
// 初期化
for (int i = 0; i < num_fds; i++) {
    fds[i].fd = client_fds[i];
    fds[i].events = POLLIN;  // 監視するイベント
}

while (1) {
    int ready = poll(fds, num_fds, -1);
    
    // やはり全部チェック
    for (int i = 0; i < num_fds; i++) {
        if (fds[i].revents & POLLIN) {
            handle_read(fds[i].fd);
        }
    }
}
```

- 可変長配列を引数に取るようになり、 fd の上限値問題を解消
- 結局 fd の数分のイテレーションが必要

### epoll の気持ち

- ここまで、全てアクティブポーリング
- イベント駆動な仕組みにできないのか？
- イベントを検知した fd だけを返す：each 文の効率化
- ユーザ空間で fd の個数分メモリ領域を取られるのも嫌！
  - 逐次的に fd をカーネル空間に送り込んで監視させたい。

#### epoll の仕組み

##### 前提：epoll の大事なオブジェクト

- red-black tree(赤黒木)：fd の管理に使用
  - fd の検索がO(log n) で終わって嬉しい
- ready list：イベントが確認された fd のリスト
- wait_queue：linux 既存のリソースに対するイベントを監視する仕組み
  - 任意のハンドラを登録して、イベント時に hook することが出来る

```c
int epfd = epoll_create1(0);
struct epoll_event ev;

// 一度だけ登録（カーネルが記憶）
ev.events = EPOLLIN;
ev.data.fd = client_fd;
epoll_ctl(epfd, EPOLL_CTL_ADD, client_fd, &ev);

struct epoll_event events[MAX_EVENTS];
while (1) {
    // 準備完了したものだけ返される
    int n = epoll_wait(epfd, events, MAX_EVENTS, -1);
    for (int i = 0; i < n; i++) {  // nだけループ(全FDではない!)
        handle_event(events[i].data.fd);
    }
}
```

- 全体の流れ図
```
┌─────────────────┐
│  epoll_ctl(ADD) │
└────────┬────────┘
         │
         ↓
┌────────────────────────────────────┐
│ カーネル内 eventpoll 構造体         │
│                                    │
│  ┌──────────────┐                  │
│  │ Red-Black    │ ← FD検索用       │
│  │ Tree         │   O(log n)       │
│  └──────────────┘                  │
│                                    │
│  ┌──────────────┐                  │
│  │ Ready List   │ ← イベント済み   │
│  │ (空)         │   FDのリスト      │
│  └──────────────┘                  │
└────────────────────────────────────┘
         │
         │ wait queue登録
         ↓
┌────────────────────┐
│  Socket/File wait  │
│  queue             │
└────────────────────┘
         ↑
         │ 割り込み
         │
    [データ到着]
         │
         ↓ wake_up()
┌────────────────────┐
│ ep_poll_callback   │ ← コールバック実行
│  → Ready Listへ追加│
│  → epoll_wait起床  │
└────────────────────┘
         │
         ↓
┌────────────────────┐
│  epoll_wait()      │
│  → Ready List取得  │
│  → ユーザー空間へ  │
└────────────────────┘

```markdown
【準備フェーズ】
1. epoll_create でepollインスタンス作成
   - red-black tree（監視対象管理用）
   - ready list（イベント済みFD管理用）を初期化

2. epoll_ctl により fd を監視対象として登録
   - red-black tree に fd を追加
   - ★重要★ fd の wait queue にコールバック関数を登録
   - このコールバックには「ready listへ追加する処理」が含まれる

3. epoll_wait の実行
   - ready list をチェック
   - 空なら自分自身のwait queueで待機（スリープ）

---【ここでプロセスは待機状態】---

【イベント発生フェーズ】
4. fd に対してイベント発生（例：ソケットへのデータ到着）

5. 割り込み発生 → 割り込みハンドラ実行
   - ネットワークカードからの割り込み
   - TCP/IPスタック処理
   - ソケットバッファへデータ書き込み

6. カーネルが wake_up() を呼び出し
   - fd の wait queue に登録されている全てのコールバックを順次実行
   - ★ここで epoll のコールバック (ep_poll_callback) が実行される

7. ep_poll_callback の中で：
   - ready list に該当 fd を追加
   - epoll_wait で待機中のプロセスを起床 (wake_up)

8. epoll_wait に復帰
   - ready list の内容をユーザー空間へコピー
   - 準備完了した fd のリストを返却
```

## ファイルをメモリへマッピングする

- read/write もしたくないよ〜
  - 読み書きの度にシステムコール発行したくないよ〜
  - ユーザ空間/カーネル空間のバッファも煩わしいよ〜
- メモリへのIOのようにシームレスに書き込みがしたいよ〜

### mmap

- 仮想アドレス空間とファイルの紐づけを行う。

![メモリ割り当てのイメージ](https://mkguytone.github.io/allocator-navigatable/img/mmap_shared.png)

- 仮想アドレス空間のIOと同様に、ファイルへのIOを行うことが出来る！
  - ディスクへの書き込みタイミングは後述。
- MAP_SHRED を指定すると、異なるプロセスの間でもファイルに紐づけたメモリ空間を紐づける事ができる
- ファイルのオフセット更新には lseek が必要だが、今回はただのメモリになるので、単なるポインタ処理でOK！

### メモリ保護属性の更新

- rwx でメモリの保護を行うことが出来る。
- mmap 時のフラグ設定によりこの設定が可能

### ファイルへの同期

- msync によりファイルへの書き出しが可能
- https://ja.manpages.org/msync/2

### メモリへのアドバイス

- マッピングしたメモリに対して、アドバイスを送ることが出来る！
- →アドバイス？
- 要は対象のメモリに関する挙動を最適化するための flag を渡しておく！
- https://kazmax.zpp.jp/cmd/m/madvise.2.html

## ファイルIOへのアドバイス

- madvise と同様、ファイルに対する posix_fadvice という syscall が存在する
- https://kazmax.zpp.jp/cmd/p/posix_fadvise.2.html

- 昔は readahead が使われ、明示的にページキャッシュにディスク内容が読み出されていた
- https://kazmax.zpp.jp/cmd/r/readahead.2.html

## 2種類の同期/非同期

同期/非同期には2種類あるんおすな〜

### synchronized/non-synchronized

- 処理の完遂に対する同期、非同期を表現する

### synchronous/asynchronous

- プログラム上での呼び出し元への復帰に対する同期、非同期を表現する

それぞれは独立であり、以下のような例示が出来る

1. Synchronous + Synchronized（同期 + 同期化）

```c
int fd = open("file.txt", O_WRONLY | O_SYNC);
write(fd, data, size);  // ブロックして、ディスクへの書き込みも待つ
close(fd);
```
**用途:** データベースのトランザクションログなど、確実性が最重要

2. Synchronous + Nonsynchronized（同期 + 非同期化）
```c
int fd = open("file.txt", O_WRONLY);
write(fd, data, size);  // ブロックするが、キャッシュへの書き込みまで
close(fd);
```
**用途:** 一般的なファイル書き込み

3. Asynchronous + Synchronized（非同期 + 同期化）

```c
int fd = open("file.txt", O_WRONLY | O_SYNC);
struct aiocb cb = { /* ... */ };
aio_write(&cb);  // すぐ戻るが、裏でディスクへの書き込みを保証
// 他の処理...
aio_suspend(&cb);  // 完了待ち
```
**用途:** 高性能が必要で、かつデータの永続性も重要な場合

4. Asynchronous + Nonsynchronized（非同期 + 非同期化）

```c
int fd = open("file.txt", O_WRONLY);
struct aiocb cb = { /* ... */ };
aio_write(&cb);  // すぐ戻り、キャッシュへの書き込みのみ
// 他の処理...
```
**用途:** 最高のパフォーマンスが必要で、多少のデータ損失リスクを許容できる場合

### asynchronous IOの発行

1. aio インターフェース

POSIX でインターフェースが提供されている
https://man7.org/linux/man-pages/man7/aio.7.html

- asynchronous なIOは別スレッドで実行することが最適な場合が多い
  - IO処理を担当するワーカースレッドを用意
  - IO要求をワーカースレッドで受け付けるインターフェースを定義
  - IOを処理し、ワーカースレッドからIO結果をエンキューする仕組みを用意
  - キューからIO結果をデキューするインターフェースをメインスレッドに提供

## IOスケジューラとIOパフォーマンス

- しょうがない…ディスクIOが必要な場合

### 物理メモリからディスクへのIOの仕組み

- HDD の仕組み

![HDD の画像](https://atmarkit.itmedia.co.jp/fwin2k/experiments/defragment/defrag_fig1.gif)

- HDD へのIOは、以下の3つのパラメータによって決定的に実行できる
  - プラッタ：どのディスクが対象か：ヘッドと一対一対応
  - トラック：同心円状のどの分割を選択するか
  - セクタ：トラック上のどの領域を選択するか
- 3つの情報は ブロック番号によってナンバリングされる。ソフトウェアはブロック番号をディスクにわたし、ディスク側でブロック番号 -> CHS番号への変換を行う。
- (ブロック番号へのアクセシビリティは inode によって担保されている)

- 特にトラック、セクタにランダムにアクセスされると、物理的にヘッドの seek に時間がかかる
- 可能な限り、ヘッドの動きを最小化したい！

### IO スケジューラ

1. merge：連続するブロック番号へのIOを merge
2. sort：ブロック番号順にIOの処理順をソート
  a. ヘッドの動きを最小化出来る！

### 読み取り処理の救済

#### 読み書きの非対称性

- 読み出し：基本的にその場でファイル内容を読み出したいので、同期処理になりがち
- 書き込み：そもそも基本的に write はディスクアクセスをしない(カーネルバッファへの書き込みまで)

- これにより…プログラム内で連続したIOをすることを考えると
- 書き込み処理は高速にIO要求が飛ぶ
- 読み出しは同期的なので、シーケンシャルな実行が必要
- →書き込み処理ばっかりやってしまう！

- また、 sort を任意のタイミングで実行すると、特定の領域に連続したIOが発生した際、ブロック番号の大きいセクタへのIOは無限に待たされることになる
- Linus エレベータ参上！
  - timeout を設けて、キューにIO要求が一定時間溜まっている場合はソートを辞める仕組みを導入
  - 暫定対応すぎワロタw

#### Deadline スケジューラ

- 設定
  - キューを3つ用意：
    - 標準キュー
    - 読み出しキュー
    - 書き込みキュー
  - キューのタイムアウト設定：
    - 読み出しキュー：500ms
    - 書き込みキュー：5s

- アルゴリズム
1. 標準キュー及びIOの種類に応じて読み出し/書き込みキューにエンキュー
2. 標準キューは常に ブロック番号 でソートし、IOを処理
3. デキューは標準キュー及びIO要求に対応するいずれかのキューから実施
4. 読み出し/書き込みキューからタイムアウトの間デキューが実行されない場合、デキューを該当のキューから一定回数連続して行う
5. ある程度処理したら標準キューに復帰

#### Anticipatory スケジューラ

- Deadline スケジューラのワーストケースを考える
- 標準キュー -> 読み出しキュー -> 標準キュー -> 読み出しキュー -> ...
- この場合、結局ヘッドの移動距離は大きい
- 連続した読み出し要求は同期的に行われるため、このケースは頻発する

そこで！

- Anticipatory スケジューラ
- 読み出しIO処理の後、6ms 次の要求を待機する
- ユースケースから、意外とこのアルゴリズムがハマるらしい

#### CFQ スケジューラ

Complete Fair Queueing

- プリエンプティブ・マルチタスクに類似するスケジューリング
- プロセス毎にキューを持ち、IO処理を時分割によって行う
- シンクロナイズド処理の優先順位を上げるため、読み出しの問題も解決する
- 大抵これでいいらしい

#### Noop スケジューラ

- merge 以外なーにもしないスケジューラ

### スケジューラの設定

- sys/block/hda/queue/scheduler で文字列管理されている

### ユーザ空間でのスケジューリング

ユーザ空間側でIOをソートしてから渡してもいいんだからねっ
(あんまり実用性はないよ)

#### パス名でのソート

- 同じディレクトリ以下のファイルに対応するブロック番号は、シーケンシャルになる傾向がある
- 逆ポで辿った順にしたらいいんじゃないですか？

#### inode 番号でのソート

- まあ inode の発行順はブロック番号に一致しがちかもね…

#### ブロック番号によるソート

- ioctl によって、fd のブロック番号を取得可能
